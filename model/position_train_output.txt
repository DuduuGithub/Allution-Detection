You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.
Some weights of BertModel were not initialized from the model checkpoint at /home/wangcs/allusion_recognition/model/guwenbert-large and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

=== 训练设备信息 ===
使用设备: cuda
GPU型号: NVIDIA TITAN Xp
当前GPU显存使用: 0.00 MB
当前GPU显存缓存: 0.00 MB
==============================

Loaded allusion dictionary with 1013 entries
Found 1014 types (including non-allusion type)
Type label 0 is reserved for non-allusion
Starting Stage 1: Position Recognition
Training for 15 epochs
Using training data: /home/wangcs/allusion_recognition/data/4_train_position.csv
Using validation data: /home/wangcs/allusion_recognition/data/4_val_position.csv
Total batches per epoch: 957
Will print progress every 50 batches
Epoch 1, Batch 50/957, Recent Average Loss: 202.9724, Current Batch Loss: 148.7466
Epoch 1, Batch 100/957, Recent Average Loss: 121.0690, Current Batch Loss: 102.2344
Epoch 1, Batch 150/957, Recent Average Loss: 107.2252, Current Batch Loss: 83.5672
Epoch 1, Batch 200/957, Recent Average Loss: 104.2607, Current Batch Loss: 102.6609
Epoch 1, Batch 250/957, Recent Average Loss: 99.7644, Current Batch Loss: 103.3329
Epoch 1, Batch 300/957, Recent Average Loss: 97.7475, Current Batch Loss: 68.5919
Epoch 1, Batch 350/957, Recent Average Loss: 95.3111, Current Batch Loss: 90.2576
Epoch 1, Batch 400/957, Recent Average Loss: 92.1903, Current Batch Loss: 100.2640
Epoch 1, Batch 450/957, Recent Average Loss: 86.8251, Current Batch Loss: 93.8503
Epoch 1, Batch 500/957, Recent Average Loss: 85.2506, Current Batch Loss: 65.5159
Epoch 1, Batch 550/957, Recent Average Loss: 82.7891, Current Batch Loss: 70.5033
Epoch 1, Batch 600/957, Recent Average Loss: 85.0813, Current Batch Loss: 80.6306
Epoch 1, Batch 650/957, Recent Average Loss: 79.0375, Current Batch Loss: 82.7516
Epoch 1, Batch 700/957, Recent Average Loss: 75.7915, Current Batch Loss: 69.6618
Epoch 1, Batch 750/957, Recent Average Loss: 76.8882, Current Batch Loss: 83.8820
Epoch 1, Batch 800/957, Recent Average Loss: 77.7433, Current Batch Loss: 85.5267
Epoch 1, Batch 850/957, Recent Average Loss: 71.0863, Current Batch Loss: 53.1425
Epoch 1, Batch 900/957, Recent Average Loss: 69.8980, Current Batch Loss: 57.7106
Epoch 1, Batch 950/957, Recent Average Loss: 68.1965, Current Batch Loss: 76.3004
Traceback (most recent call last):
  File "/home/wangcs/allusion_recognition/model/train.py", line 426, in <module>
    main()
  File "/home/wangcs/allusion_recognition/model/train.py", line 378, in main
    train_model(
  File "/home/wangcs/allusion_recognition/model/train.py", line 181, in train_model
    predictions = model(
                  ^^^^^^
  File "/home/wangcs/conda_env/torch_deepleanrning/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wangcs/conda_env/torch_deepleanrning/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wangcs/allusion_recognition/model/bert_crf.py", line 175, in forward
    prediction = self.position_crf.viterbi_decode(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wangcs/conda_env/torch_deepleanrning/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'CRF' object has no attribute 'viterbi_decode'. Did you mean: '_viterbi_decode'?
