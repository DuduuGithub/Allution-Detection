You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.
Some weights of BertModel were not initialized from the model checkpoint at /home/wangcs/allusion_recognition/model/guwenbert-large and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.16.output.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/wangcs/allusion_recognition/model_for_new_study/bert_crf.py:205: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(1 + self.bi_label_weight, device=labels.device),

=== 训练设备信息 ===
使用设备: cuda
GPU型号: NVIDIA TITAN Xp
当前GPU显存使用: 0.00 MB
当前GPU显存缓存: 0.00 MB
==============================

Loaded allusion dictionary with 1521 entries
Found 1522 types (including non-allusion type)
Type label 0 is reserved for non-allusion

starting from scratch
这是正则化损失后的测试，将positionweight为1.0000，bi_label_weight为0.1500
Starting training with 25 epochs
Training samples: 27493
Validation samples: 3446
Epoch 1/25
Position Weight (Joint Loss): 1.0000
B/I Label Weight: 0.1500
Epoch 1/25 - Batch 100/1719:
  Position Loss: 231.0747
  Type Loss: 73.4811
  Total Loss: 231.0747
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 200/1719:
  Position Loss: 220.4783
  Type Loss: 73.7619
  Total Loss: 220.4783
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 300/1719:
  Position Loss: 69.8150
  Type Loss: 73.9834
  Total Loss: 69.8150
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 400/1719:
  Position Loss: 44.6247
  Type Loss: 75.9665
  Total Loss: 44.6247
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '贱子曾尘国士知，登门倒屣忆当时。' -> Matched: '贱子曾尘国士知，登门倒屣忆当时。 '
Epoch 1/25 - Batch 500/1719:
  Position Loss: 14.4458
  Type Loss: 73.9059
  Total Loss: 14.4458
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 600/1719:
  Position Loss: 11.0883
  Type Loss: 74.2047
  Total Loss: 11.0883
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 700/1719:
  Position Loss: 9.2158
  Type Loss: 72.5943
  Total Loss: 9.2158
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 800/1719:
  Position Loss: 4.1167
  Type Loss: 72.8264
  Total Loss: 4.1167
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 900/1719:
  Position Loss: 9.8927
  Type Loss: 73.6582
  Total Loss: 9.8927
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1000/1719:
  Position Loss: 10.9515
  Type Loss: 75.1181
  Total Loss: 10.9515
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。' -> Matched: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。 '
Epoch 1/25 - Batch 1100/1719:
  Position Loss: 7.4905
  Type Loss: 77.3885
  Total Loss: 7.4905
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1200/1719:
  Position Loss: 3.8264
  Type Loss: 73.5511
  Total Loss: 3.8264
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1300/1719:
  Position Loss: 9.8818
  Type Loss: 74.2106
  Total Loss: 9.8818
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '离堂莫起临歧叹，文举终当荐祢衡。' -> Matched: ' 离堂莫起临歧叹，文举终当荐祢衡。'
Epoch 1/25 - Batch 1400/1719:
  Position Loss: 3.4876
  Type Loss: 75.0114
  Total Loss: 3.4876
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1500/1719:
  Position Loss: 4.0967
  Type Loss: 74.7020
  Total Loss: 4.0967
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1600/1719:
  Position Loss: 7.1269
  Type Loss: 72.6939
  Total Loss: 7.1269
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 1/25 - Batch 1700/1719:
  Position Loss: 4.1452
  Type Loss: 75.0203
  Total Loss: 4.1452
  Position Weight: 1.0000
  B/I Label Weight: 0.1500

Validation Scores:
Position Score (B/I F1): 0.7354
Type Score (Top1 Acc): 0.0011
Combined Score: 0.4417

Epoch 1/25 Summary:
Training Loss:
  Position Loss: 43.6103
  Type Loss: 74.1769
  Total Loss: 43.6103
Validation Loss:
  Position Loss: 5.3314
  Type Loss: 74.0225
  Total Loss: 5.3314

Performance Metrics:
Position Recognition:
  Overall:
    Precision: 0.6998
    Recall: 0.8000
    F1: 0.7465
  B Label:
    Precision: 0.6647
    Recall: 0.7447
    F1: 0.7024
    TP: 2730, FP: 1377, FN: 936
  I Label:
    Precision: 0.7169
    Recall: 0.8278
    F1: 0.7684
    TP: 6030, FP: 2381, FN: 1254
  O Label:
    Precision: 0.9694
    Recall: 0.9430
    F1: 0.9560
    TP: 54301, FP: 1715, FN: 3283
Type Recognition:
  Top-1 Accuracy: 0.0011
  Top-3 Accuracy: 0.0026
  Top-5 Accuracy: 0.0034
  Mistake:
    Positive to Negative: 0.9966
    Negative to Positive: 1.0000
raw_data:
    Positive Correct: 4
    Positive Total: 3489
    Positive Top3 Correct: 9
    Positive Top5 Correct: 12
    Negative Total: 182
    Negative Correct: 0
==================================================
Epoch 2/25
Position Weight (Joint Loss): 1.0000
B/I Label Weight: 0.1500
Epoch 2/25 - Batch 100/1719:
  Position Loss: 4.4136
  Type Loss: 73.2308
  Total Loss: 4.4136
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '离堂莫起临歧叹，文举终当荐祢衡。' -> Matched: ' 离堂莫起临歧叹，文举终当荐祢衡。'
Epoch 2/25 - Batch 200/1719:
  Position Loss: 8.7079
  Type Loss: 74.0014
  Total Loss: 8.7079
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 300/1719:
  Position Loss: 2.5230
  Type Loss: 74.9684
  Total Loss: 2.5230
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 400/1719:
  Position Loss: 1.9917
  Type Loss: 75.7875
  Total Loss: 1.9917
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 500/1719:
  Position Loss: 4.7438
  Type Loss: 75.6365
  Total Loss: 4.7438
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 600/1719:
  Position Loss: 8.8515
  Type Loss: 72.9915
  Total Loss: 8.8515
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 700/1719:
  Position Loss: 4.0626
  Type Loss: 75.2981
  Total Loss: 4.0626
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '贱子曾尘国士知，登门倒屣忆当时。' -> Matched: '贱子曾尘国士知，登门倒屣忆当时。 '
Epoch 2/25 - Batch 800/1719:
  Position Loss: 3.6459
  Type Loss: 74.4226
  Total Loss: 3.6459
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 900/1719:
  Position Loss: 6.8695
  Type Loss: 73.8238
  Total Loss: 6.8695
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1000/1719:
  Position Loss: 5.4143
  Type Loss: 72.8034
  Total Loss: 5.4143
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1100/1719:
  Position Loss: 3.5902
  Type Loss: 74.8508
  Total Loss: 3.5902
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1200/1719:
  Position Loss: 3.5954
  Type Loss: 75.4110
  Total Loss: 3.5954
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1300/1719:
  Position Loss: 3.1523
  Type Loss: 74.2441
  Total Loss: 3.1523
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1400/1719:
  Position Loss: 4.2919
  Type Loss: 75.2549
  Total Loss: 4.2919
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。' -> Matched: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。 '
Epoch 2/25 - Batch 1500/1719:
  Position Loss: 3.9885
  Type Loss: 74.5029
  Total Loss: 3.9885
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1600/1719:
  Position Loss: 10.2809
  Type Loss: 75.0336
  Total Loss: 10.2809
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 2/25 - Batch 1700/1719:
  Position Loss: 5.2409
  Type Loss: 74.2834
  Total Loss: 5.2409
  Position Weight: 1.0000
  B/I Label Weight: 0.1500

Validation Scores:
Position Score (B/I F1): 0.7203
Type Score (Top1 Acc): 0.0023
Combined Score: 0.4331

Epoch 2/25 Summary:
Training Loss:
  Position Loss: 4.9748
  Type Loss: 74.1494
  Total Loss: 4.9748
Validation Loss:
  Position Loss: 4.7593
  Type Loss: 73.9084
  Total Loss: 4.7593

Performance Metrics:
Position Recognition:
  Overall:
    Precision: 0.6315
    Recall: 0.8651
    F1: 0.7301
  B Label:
    Precision: 0.6216
    Recall: 0.7837
    F1: 0.6933
    TP: 2873, FP: 1749, FN: 793
  I Label:
    Precision: 0.6360
    Recall: 0.9061
    F1: 0.7474
    TP: 6600, FP: 3778, FN: 684
  O Label:
    Precision: 0.9830
    Recall: 0.9138
    F1: 0.9471
    TP: 52622, FP: 912, FN: 4962
Type Recognition:
  Top-1 Accuracy: 0.0023
  Top-3 Accuracy: 0.0031
  Top-5 Accuracy: 0.0046
  Mistake:
    Positive to Negative: 0.9954
    Negative to Positive: 1.0000
raw_data:
    Positive Correct: 8
    Positive Total: 3495
    Positive Top3 Correct: 11
    Positive Top5 Correct: 16
    Negative Total: 176
    Negative Correct: 0
==================================================
Epoch 3/25
Position Weight (Joint Loss): 1.0000
B/I Label Weight: 0.1500
Epoch 3/25 - Batch 100/1719:
  Position Loss: 11.2640
  Type Loss: 76.3138
  Total Loss: 11.2640
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 200/1719:
  Position Loss: 8.2140
  Type Loss: 73.7325
  Total Loss: 8.2140
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 300/1719:
  Position Loss: 6.9070
  Type Loss: 74.1480
  Total Loss: 6.9070
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 400/1719:
  Position Loss: 3.2311
  Type Loss: 74.0828
  Total Loss: 3.2311
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '贱子曾尘国士知，登门倒屣忆当时。' -> Matched: '贱子曾尘国士知，登门倒屣忆当时。 '
Epoch 3/25 - Batch 500/1719:
  Position Loss: 3.3027
  Type Loss: 75.7009
  Total Loss: 3.3027
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '离堂莫起临歧叹，文举终当荐祢衡。' -> Matched: ' 离堂莫起临歧叹，文举终当荐祢衡。'
Epoch 3/25 - Batch 600/1719:
  Position Loss: 3.9917
  Type Loss: 75.4128
  Total Loss: 3.9917
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 700/1719:
  Position Loss: 7.4348
  Type Loss: 74.6293
  Total Loss: 7.4348
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 800/1719:
  Position Loss: 3.8439
  Type Loss: 73.8903
  Total Loss: 3.8439
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 900/1719:
  Position Loss: 3.1332
  Type Loss: 73.0316
  Total Loss: 3.1332
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1000/1719:
  Position Loss: 2.7599
  Type Loss: 73.5533
  Total Loss: 2.7599
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1100/1719:
  Position Loss: 2.7575
  Type Loss: 74.5276
  Total Loss: 2.7575
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1200/1719:
  Position Loss: 2.1248
  Type Loss: 72.5024
  Total Loss: 2.1248
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1300/1719:
  Position Loss: 6.7782
  Type Loss: 73.9400
  Total Loss: 6.7782
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1400/1719:
  Position Loss: 10.1081
  Type Loss: 72.4300
  Total Loss: 10.1081
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。' -> Matched: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。 '
Epoch 3/25 - Batch 1500/1719:
  Position Loss: 2.5189
  Type Loss: 75.0118
  Total Loss: 2.5189
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1600/1719:
  Position Loss: 2.8705
  Type Loss: 74.5983
  Total Loss: 2.8705
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 3/25 - Batch 1700/1719:
  Position Loss: 2.7836
  Type Loss: 74.3131
  Total Loss: 2.7836
  Position Weight: 1.0000
  B/I Label Weight: 0.1500

Validation Scores:
Position Score (B/I F1): 0.7552
Type Score (Top1 Acc): 0.0009
Combined Score: 0.4535

Epoch 3/25 Summary:
Training Loss:
  Position Loss: 4.4631
  Type Loss: 74.0777
  Total Loss: 4.4631
Validation Loss:
  Position Loss: 4.4194
  Type Loss: 73.9408
  Total Loss: 4.4194

Performance Metrics:
Position Recognition:
  Overall:
    Precision: 0.6917
    Recall: 0.8533
    F1: 0.7641
  B Label:
    Precision: 0.6816
    Recall: 0.7861
    F1: 0.7302
    TP: 2882, FP: 1346, FN: 784
  I Label:
    Precision: 0.6963
    Recall: 0.8871
    F1: 0.7802
    TP: 6462, FP: 2819, FN: 822
  O Label:
    Precision: 0.9793
    Recall: 0.9357
    F1: 0.9570
    TP: 53884, FP: 1141, FN: 3700
Type Recognition:
  Top-1 Accuracy: 0.0009
  Top-3 Accuracy: 0.0014
  Top-5 Accuracy: 0.0037
  Mistake:
    Positive to Negative: 0.9963
    Negative to Positive: 1.0000
raw_data:
    Positive Correct: 3
    Positive Total: 3503
    Positive Top3 Correct: 5
    Positive Top5 Correct: 13
    Negative Total: 168
    Negative Correct: 0
==================================================
Epoch 4/25
Position Weight (Joint Loss): 1.0000
B/I Label Weight: 0.1500
Found match with different spacing. Original: '离堂莫起临歧叹，文举终当荐祢衡。' -> Matched: ' 离堂莫起临歧叹，文举终当荐祢衡。'
Epoch 4/25 - Batch 100/1719:
  Position Loss: 2.7127
  Type Loss: 73.0384
  Total Loss: 2.7127
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 200/1719:
  Position Loss: 3.4986
  Type Loss: 74.2335
  Total Loss: 3.4986
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 300/1719:
  Position Loss: 2.5482
  Type Loss: 73.6771
  Total Loss: 2.5482
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 400/1719:
  Position Loss: 3.4080
  Type Loss: 72.9677
  Total Loss: 3.4080
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 500/1719:
  Position Loss: 6.4575
  Type Loss: 74.5240
  Total Loss: 6.4575
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 600/1719:
  Position Loss: 2.7573
  Type Loss: 74.2395
  Total Loss: 2.7573
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 700/1719:
  Position Loss: 5.9001
  Type Loss: 74.5087
  Total Loss: 5.9001
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 800/1719:
  Position Loss: 5.2299
  Type Loss: 73.6260
  Total Loss: 5.2299
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。' -> Matched: '公沙穆来游太学,无资粮,乃变服客佣,为吴祐赁舂。 '
Epoch 4/25 - Batch 900/1719:
  Position Loss: 4.2314
  Type Loss: 74.9916
  Total Loss: 4.2314
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1000/1719:
  Position Loss: 3.3169
  Type Loss: 75.4724
  Total Loss: 3.3169
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1100/1719:
  Position Loss: 5.1276
  Type Loss: 74.9532
  Total Loss: 5.1276
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1200/1719:
  Position Loss: 3.4793
  Type Loss: 73.1199
  Total Loss: 3.4793
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1300/1719:
  Position Loss: 2.6961
  Type Loss: 74.9273
  Total Loss: 2.6961
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1400/1719:
  Position Loss: 3.0855
  Type Loss: 74.0243
  Total Loss: 3.0855
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1500/1719:
  Position Loss: 5.5072
  Type Loss: 72.5214
  Total Loss: 5.5072
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1600/1719:
  Position Loss: 4.6110
  Type Loss: 73.9672
  Total Loss: 4.6110
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Epoch 4/25 - Batch 1700/1719:
  Position Loss: 3.4405
  Type Loss: 73.6596
  Total Loss: 3.4405
  Position Weight: 1.0000
  B/I Label Weight: 0.1500
Found match with different spacing. Original: '贱子曾尘国士知，登门倒屣忆当时。' -> Matched: '贱子曾尘国士知，登门倒屣忆当时。 '

Validation Scores:
Position Score (B/I F1): 0.7561
Type Score (Top1 Acc): 0.0003
Combined Score: 0.4538
Early stopping triggered at epoch 4.
Best Combined Score: -1.0000
