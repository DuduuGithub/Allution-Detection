{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬完了第1页的数据，本页共有1207条数据\n",
      "爬完了第2页的数据，本页共有707条数据\n",
      "爬完了第3页的数据，本页共有593条数据\n",
      "爬完了第4页的数据，本页共有491条数据\n",
      "爬完了第5页的数据，本页共有446条数据\n",
      "爬完了第6页的数据，本页共有420条数据\n",
      "爬完了第7页的数据，本页共有390条数据\n",
      "爬完了第8页的数据，本页共有364条数据\n",
      "爬完了第9页的数据，本页共有338条数据\n",
      "爬完了第10页的数据，本页共有324条数据\n",
      "爬完了第11页的数据，本页共有299条数据\n",
      "爬完了第12页的数据，本页共有283条数据\n",
      "爬完了第13页的数据，本页共有273条数据\n",
      "爬完了第14页的数据，本页共有258条数据\n",
      "爬完了第15页的数据，本页共有242条数据\n",
      "爬完了第16页的数据，本页共有233条数据\n",
      "爬完了第17页的数据，本页共有222条数据\n",
      "爬完了第18页的数据，本页共有211条数据\n",
      "爬完了第19页的数据，本页共有204条数据\n",
      "爬完了第20页的数据，本页共有198条数据\n",
      "爬完了第21页的数据，本页共有190条数据\n",
      "爬完了第22页的数据，本页共有186条数据\n",
      "爬完了第23页的数据，本页共有180条数据\n",
      "爬完了第24页的数据，本页共有177条数据\n",
      "爬完了第25页的数据，本页共有170条数据\n",
      "爬完了第26页的数据，本页共有167条数据\n",
      "爬完了第27页的数据，本页共有160条数据\n",
      "爬完了第28页的数据，本页共有159条数据\n",
      "爬完了第29页的数据，本页共有150条数据\n",
      "爬完了第30页的数据，本页共有150条数据\n",
      "爬完了第31页的数据，本页共有146条数据\n",
      "爬完了第32页的数据，本页共有140条数据\n",
      "爬完了第33页的数据，本页共有140条数据\n",
      "爬完了第34页的数据，本页共有132条数据\n",
      "爬完了第35页的数据，本页共有130条数据\n",
      "爬完了第36页的数据，本页共有127条数据\n",
      "爬完了第37页的数据，本页共有120条数据\n",
      "爬完了第38页的数据，本页共有120条数据\n",
      "爬完了第39页的数据，本页共有120条数据\n",
      "爬完了第40页的数据，本页共有115条数据\n",
      "爬完了第41页的数据，本页共有110条数据\n",
      "爬完了第42页的数据，本页共有110条数据\n",
      "爬完了第43页的数据，本页共有110条数据\n",
      "爬完了第44页的数据，本页共有108条数据\n",
      "爬完了第45页的数据，本页共有100条数据\n",
      "爬完了第46页的数据，本页共有100条数据\n",
      "爬完了第47页的数据，本页共有100条数据\n",
      "爬完了第48页的数据，本页共有95条数据\n",
      "爬完了第49页的数据，本页共有90条数据\n",
      "爬完了第50页的数据，本页共有90条数据\n",
      "爬完了第51页的数据，本页共有90条数据\n",
      "爬完了第52页的数据，本页共有90条数据\n",
      "爬完了第53页的数据，本页共有90条数据\n",
      "爬完了第54页的数据，本页共有81条数据\n",
      "爬完了第55页的数据，本页共有80条数据\n",
      "爬完了第56页的数据，本页共有80条数据\n",
      "爬完了第57页的数据，本页共有80条数据\n",
      "爬完了第58页的数据，本页共有80条数据\n",
      "爬完了第59页的数据，本页共有80条数据\n",
      "爬完了第60页的数据，本页共有80条数据\n",
      "爬完了第61页的数据，本页共有80条数据\n",
      "爬完了第62页的数据，本页共有76条数据\n",
      "爬完了第63页的数据，本页共有70条数据\n",
      "爬完了第64页的数据，本页共有70条数据\n",
      "爬完了第65页的数据，本页共有70条数据\n",
      "爬完了第66页的数据，本页共有70条数据\n",
      "爬完了第67页的数据，本页共有70条数据\n",
      "爬完了第68页的数据，本页共有70条数据\n",
      "爬完了第69页的数据，本页共有70条数据\n",
      "爬完了第70页的数据，本页共有70条数据\n",
      "爬完了第71页的数据，本页共有69条数据\n",
      "爬完了第72页的数据，本页共有60条数据\n",
      "爬完了第73页的数据，本页共有60条数据\n",
      "爬完了第74页的数据，本页共有60条数据\n",
      "爬完了第75页的数据，本页共有60条数据\n",
      "爬完了第76页的数据，本页共有60条数据\n",
      "爬完了第77页的数据，本页共有60条数据\n",
      "爬完了第78页的数据，本页共有60条数据\n",
      "爬完了第79页的数据，本页共有60条数据\n",
      "爬完了第80页的数据，本页共有60条数据\n",
      "爬完了第81页的数据，本页共有60条数据\n",
      "爬完了第82页的数据，本页共有60条数据\n",
      "爬完了第83页的数据，本页共有60条数据\n",
      "爬完了第84页的数据，本页共有60条数据\n",
      "爬完了第85页的数据，本页共有51条数据\n",
      "爬完了第86页的数据，本页共有50条数据\n",
      "爬完了第87页的数据，本页共有50条数据\n",
      "爬完了第88页的数据，本页共有50条数据\n",
      "爬完了第89页的数据，本页共有50条数据\n",
      "爬完了第90页的数据，本页共有50条数据\n",
      "爬完了第91页的数据，本页共有50条数据\n",
      "爬完了第92页的数据，本页共有50条数据\n",
      "爬完了第93页的数据，本页共有50条数据\n",
      "爬完了第94页的数据，本页共有50条数据\n",
      "爬完了第95页的数据，本页共有50条数据\n",
      "爬完了第96页的数据，本页共有50条数据\n",
      "爬完了第97页的数据，本页共有50条数据\n",
      "爬完了第98页的数据，本页共有50条数据\n",
      "爬完了第99页的数据，本页共有50条数据\n",
      "爬完了第100页的数据，本页共有50条数据\n",
      "爬完了第101页的数据，本页共有50条数据\n",
      "爬完了第102页的数据，本页共有30条数据\n",
      "共有14972条数据\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_variations_list(content):\n",
    "    # 获取典故的 allusion_block_list 的列表\n",
    "    allusion_block_list = content.find_all(name=\"div\", attrs={\"class\": \"allusionBlock\"})\n",
    "\n",
    "    variation_list = []\n",
    "\n",
    "    for child in allusion_block_list[0]:\n",
    "        if isinstance(child, str):  # 跳过文本节点\n",
    "            continue\n",
    "        previous_siblings = child.find_previous_siblings()\n",
    "\n",
    "        # print(f\"当前元素: {child.name} - {child.text}\")\n",
    "        # print(\"前面的兄弟元素:\")\n",
    "        flag = True\n",
    "\n",
    "        # 如果元素含有 “典故” 、 “相关人物” 或 “参考典故” ，那么不要这个元素\n",
    "        if((child.get_text()==\"典故\") | (child.get_text()==\"相关人物\") | (child.get_text()==\"参考典故\") | (child.get_text()==\"\")):\n",
    "            flag = False\n",
    "\n",
    "        for sibling in previous_siblings:\n",
    "            # print(f\"  {sibling.name} - {sibling.text}\")\n",
    "            # 如果前面的元素有 “相关人物” 和 “参考典故” ，那么不要这个元素\n",
    "            if((sibling.get_text()==\"相关人物\") | (sibling.get_text()==\"参考典故\")):\n",
    "                flag = False\n",
    "        if(flag):\n",
    "            variation_list.append(child.get_text())\n",
    "\n",
    "        #print('-' * 50)\n",
    "    # print(variation_list)\n",
    "    # print(len(variation_list))\n",
    "    return variation_list\n",
    "\n",
    "def get_poem_list(content):\n",
    "    # 获取典故的 allusion_block_list 的列表\n",
    "    allusion_block_list = content.find_all(name=\"div\", attrs={\"class\": \"allusionBlock\"})\n",
    "\n",
    "    poem_list = []\n",
    "\n",
    "    child_list = allusion_block_list[-1].find_all(name=\"p\")\n",
    "    for child in child_list:\n",
    "        poem_dict = {}\n",
    "        # print(child.contents[0])\n",
    "        sentence = child.contents[0].strip()\n",
    "        if(child.find(name=\"span\")):\n",
    "            # print(child.find(name=\"span\").contents[0])\n",
    "            author = child.find(name=\"span\").contents[0].strip()\n",
    "            if(child.find(name=\"a\")):\n",
    "                title = child.find(name=\"span\").contents[1].get_text().strip()\n",
    "                poem_dict[\"sentence\"] = sentence\n",
    "                poem_dict[\"author\"] = author\n",
    "                poem_dict[\"title\"] = title\n",
    "                poem_list.append(poem_dict)\n",
    "            else:\n",
    "                poem_dict[\"sentence\"] = sentence\n",
    "                poem_dict[\"author\"] = author.split()[0].strip()\n",
    "                poem_dict[\"title\"] = author.split()[1].strip()\n",
    "                poem_list.append(poem_dict)\n",
    "    # print(poem_list)\n",
    "    # print(len(poem_list))\n",
    "    return poem_list\n",
    "\n",
    "def filter_contained_lists(lst):\n",
    "    result = []\n",
    "    for i, sublist1 in enumerate(lst):\n",
    "        is_contained = False\n",
    "        for j, sublist2 in enumerate(lst):\n",
    "            if i != j and set(sublist1).issubset(set(sublist2)):\n",
    "                is_contained = True\n",
    "                break\n",
    "        if not is_contained:\n",
    "            result.append(sublist1)\n",
    "    return result\n",
    "\n",
    "def get_added_poem_list(allusion, variation_list, poem_list):\n",
    "    # 遍历这个典故的所有诗句\n",
    "    for poem_dict in poem_list:\n",
    "\n",
    "        # 设置该诗句的典故\n",
    "        poem_dict[\"allusion\"] = allusion\n",
    "\n",
    "        # 初始化该诗句涉及到的典故的异形的数量为 0\n",
    "        poem_dict[\"variation_number\"] = 0\n",
    "\n",
    "        # 初始化该诗句的典故异性的位置列表\n",
    "        poem_dict[\"allusion_index\"] = []\n",
    "\n",
    "        # 遍历这个典故的异形列表\n",
    "        for variation in variation_list:\n",
    "            # 如果该诗句包含本异形 variation\n",
    "            if(variation in poem_dict[\"sentence\"]):\n",
    "                # 将该异形的位置加到 index列表里面\n",
    "                poem_dict[\"allusion_index\"].append(list(range(poem_dict[\"sentence\"].find(variation), poem_dict[\"sentence\"].find(variation) + len(variation))))\n",
    "                poem_dict[\"variation_number\"]+=1\n",
    "        # if(poem_dict[\"variation_number\"]==0):\n",
    "        #     print(str(poem_dict) + \"\\t\" + \"出现典故大于一次\")\n",
    "        poem_dict['allusion_index'] = filter_contained_lists(poem_dict['allusion_index'])\n",
    "    return poem_list\n",
    "\n",
    "def process_one_page(url_of_page,url_of_file):\n",
    "    # 获取HTML文本\n",
    "    res = requests.get(\n",
    "        url_of_page\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(res.text, features=\"html.parser\")\n",
    "\n",
    "    # 获取典故名字列表\n",
    "    allusion_list = [x.get_text() for x in soup.find(attrs={\"id\": \"IndexPanel\"}).find_all(name=\"a\")]\n",
    "\n",
    "    # 获取 ContentPanel 列表\n",
    "    content_list = soup.find(attrs={\"id\": \"ContentPanel\"}).find_all(attrs={\"class\": \"allusion\"})\n",
    "\n",
    "    if_have_all_poem_df = False\n",
    "\n",
    "    # 遍历 allusion_list 和 content_list ，即遍历这一页的所有典故\n",
    "    for allusion, content in zip(allusion_list, content_list):\n",
    "\n",
    "        # 获取该典故的异形的列表\n",
    "        variation_list = get_variations_list(content)\n",
    "\n",
    "        # 获取该典故的所有诗句的列表，列表元素是字典\n",
    "        poem_list = get_poem_list(content)\n",
    "        \n",
    "        # 对该典故的所有诗句里面的词的位置进行标注\n",
    "        poem_list = get_added_poem_list(allusion, variation_list, poem_list)\n",
    "        \n",
    "        temp_df = pd.DataFrame(poem_list)\n",
    "    \n",
    "        if(if_have_all_poem_df==False):\n",
    "            if_have_all_poem_df = True\n",
    "            all_poem_df = temp_df\n",
    "        else:\n",
    "            all_poem_df = pd.concat([all_poem_df,temp_df], ignore_index=True)# ignore_index 参数用于重新设置索引\n",
    "    # print(all_poem_df[[\"allusion_index\"]].head())\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(url_of_file):\n",
    "        # 如果文件存在，追加数据时不保存列名\n",
    "        all_poem_df.to_csv(url_of_file, sep='\\t', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        # 如果文件不存在，保存数据并保存列名\n",
    "        all_poem_df.to_csv(url_of_file, sep='\\t', index=False)\n",
    "    return all_poem_df.shape[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_len = 0\n",
    "    for i in range(0,102,1):\n",
    "        url_of_page = \"https://sou-yun.cn/AllusionsIndex.aspx?page=\" + str(i)\n",
    "        url_of_file = \"D:/软件包/爬取的典故数据.csv\"\n",
    "        this_page_len = process_one_page(url_of_page,url_of_file)\n",
    "        all_len+=this_page_len\n",
    "        print(\"爬完了第\" + str(i+1) + \"页的数据，本页共有\" + str(this_page_len) + \"条数据\")\n",
    "    print(\"共有\" + str(all_len) + \"条数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬完了第1页的数据，本页共有10条数据\n",
      "爬完了第2页的数据，本页共有10条数据\n",
      "爬完了第3页的数据，本页共有10条数据\n",
      "爬完了第4页的数据，本页共有10条数据\n",
      "爬完了第5页的数据，本页共有10条数据\n",
      "爬完了第6页的数据，本页共有10条数据\n",
      "爬完了第7页的数据，本页共有10条数据\n",
      "爬完了第8页的数据，本页共有10条数据\n",
      "爬完了第9页的数据，本页共有10条数据\n",
      "爬完了第10页的数据，本页共有10条数据\n",
      "爬完了第11页的数据，本页共有10条数据\n",
      "爬完了第12页的数据，本页共有10条数据\n",
      "爬完了第13页的数据，本页共有10条数据\n",
      "爬完了第14页的数据，本页共有10条数据\n",
      "爬完了第15页的数据，本页共有10条数据\n",
      "爬完了第16页的数据，本页共有10条数据\n",
      "爬完了第17页的数据，本页共有10条数据\n",
      "爬完了第18页的数据，本页共有10条数据\n",
      "爬完了第19页的数据，本页共有10条数据\n",
      "爬完了第20页的数据，本页共有10条数据\n",
      "爬完了第21页的数据，本页共有10条数据\n",
      "爬完了第22页的数据，本页共有10条数据\n",
      "爬完了第23页的数据，本页共有10条数据\n",
      "爬完了第24页的数据，本页共有10条数据\n",
      "爬完了第25页的数据，本页共有10条数据\n",
      "爬完了第26页的数据，本页共有10条数据\n",
      "爬完了第27页的数据，本页共有10条数据\n",
      "爬完了第28页的数据，本页共有10条数据\n",
      "爬完了第29页的数据，本页共有10条数据\n",
      "爬完了第30页的数据，本页共有10条数据\n",
      "爬完了第31页的数据，本页共有10条数据\n",
      "爬完了第32页的数据，本页共有10条数据\n",
      "爬完了第33页的数据，本页共有10条数据\n",
      "爬完了第34页的数据，本页共有10条数据\n",
      "爬完了第35页的数据，本页共有10条数据\n",
      "爬完了第36页的数据，本页共有10条数据\n",
      "爬完了第37页的数据，本页共有10条数据\n",
      "爬完了第38页的数据，本页共有10条数据\n",
      "爬完了第39页的数据，本页共有10条数据\n",
      "爬完了第40页的数据，本页共有10条数据\n",
      "爬完了第41页的数据，本页共有10条数据\n",
      "爬完了第42页的数据，本页共有10条数据\n",
      "爬完了第43页的数据，本页共有10条数据\n",
      "爬完了第44页的数据，本页共有10条数据\n",
      "爬完了第45页的数据，本页共有10条数据\n",
      "爬完了第46页的数据，本页共有10条数据\n",
      "爬完了第47页的数据，本页共有10条数据\n",
      "爬完了第48页的数据，本页共有10条数据\n",
      "爬完了第49页的数据，本页共有10条数据\n",
      "爬完了第50页的数据，本页共有10条数据\n",
      "爬完了第51页的数据，本页共有10条数据\n",
      "爬完了第52页的数据，本页共有10条数据\n",
      "爬完了第53页的数据，本页共有10条数据\n",
      "爬完了第54页的数据，本页共有10条数据\n",
      "爬完了第55页的数据，本页共有10条数据\n",
      "爬完了第56页的数据，本页共有10条数据\n",
      "爬完了第57页的数据，本页共有10条数据\n",
      "爬完了第58页的数据，本页共有10条数据\n",
      "爬完了第59页的数据，本页共有10条数据\n",
      "爬完了第60页的数据，本页共有10条数据\n",
      "爬完了第61页的数据，本页共有10条数据\n",
      "爬完了第62页的数据，本页共有10条数据\n",
      "爬完了第63页的数据，本页共有10条数据\n",
      "爬完了第64页的数据，本页共有10条数据\n",
      "爬完了第65页的数据，本页共有10条数据\n",
      "爬完了第66页的数据，本页共有10条数据\n",
      "爬完了第67页的数据，本页共有10条数据\n",
      "爬完了第68页的数据，本页共有10条数据\n",
      "爬完了第69页的数据，本页共有10条数据\n",
      "爬完了第70页的数据，本页共有10条数据\n",
      "爬完了第71页的数据，本页共有10条数据\n",
      "爬完了第72页的数据，本页共有10条数据\n",
      "爬完了第73页的数据，本页共有10条数据\n",
      "爬完了第74页的数据，本页共有10条数据\n",
      "爬完了第75页的数据，本页共有10条数据\n",
      "爬完了第76页的数据，本页共有10条数据\n",
      "爬完了第77页的数据，本页共有10条数据\n",
      "爬完了第78页的数据，本页共有10条数据\n",
      "爬完了第79页的数据，本页共有10条数据\n",
      "爬完了第80页的数据，本页共有10条数据\n",
      "爬完了第81页的数据，本页共有10条数据\n",
      "爬完了第82页的数据，本页共有10条数据\n",
      "爬完了第83页的数据，本页共有10条数据\n",
      "爬完了第84页的数据，本页共有10条数据\n",
      "爬完了第85页的数据，本页共有10条数据\n",
      "爬完了第86页的数据，本页共有10条数据\n",
      "爬完了第87页的数据，本页共有10条数据\n",
      "爬完了第88页的数据，本页共有10条数据\n",
      "爬完了第89页的数据，本页共有10条数据\n",
      "爬完了第90页的数据，本页共有10条数据\n",
      "爬完了第91页的数据，本页共有10条数据\n",
      "爬完了第92页的数据，本页共有10条数据\n",
      "爬完了第93页的数据，本页共有10条数据\n",
      "爬完了第94页的数据，本页共有10条数据\n",
      "爬完了第95页的数据，本页共有10条数据\n",
      "爬完了第96页的数据，本页共有10条数据\n",
      "爬完了第97页的数据，本页共有10条数据\n",
      "爬完了第98页的数据，本页共有10条数据\n",
      "爬完了第99页的数据，本页共有10条数据\n",
      "爬完了第100页的数据，本页共有10条数据\n",
      "爬完了第101页的数据，本页共有10条数据\n",
      "爬完了第102页的数据，本页共有6条数据\n",
      "共有1016条数据\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_variations_list(content):\n",
    "    # 获取典故的 allusion_block_list 的列表\n",
    "    allusion_block_list = content.find_all(name=\"div\", attrs={\"class\": \"allusionBlock\"})\n",
    "\n",
    "    variation_list = []\n",
    "\n",
    "    for child in allusion_block_list[0]:\n",
    "        if isinstance(child, str):  # 跳过文本节点\n",
    "            continue\n",
    "        previous_siblings = child.find_previous_siblings()\n",
    "\n",
    "        # print(f\"当前元素: {child.name} - {child.text}\")\n",
    "        # print(\"前面的兄弟元素:\")\n",
    "        flag = True\n",
    "\n",
    "        # 如果元素含有 “典故” 、 “相关人物” 或 “参考典故” ，那么不要这个元素\n",
    "        if((child.get_text()==\"典故\") | (child.get_text()==\"相关人物\") | (child.get_text()==\"参考典故\") | (child.get_text()==\"\")):\n",
    "            flag = False\n",
    "\n",
    "        for sibling in previous_siblings:\n",
    "            # print(f\"  {sibling.name} - {sibling.text}\")\n",
    "            # 如果前面的元素有 “相关人物” 和 “参考典故” ，那么不要这个元素\n",
    "            if((sibling.get_text()==\"相关人物\") | (sibling.get_text()==\"参考典故\")):\n",
    "                flag = False\n",
    "        if(flag):\n",
    "            variation_list.append(child.get_text())\n",
    "\n",
    "        #print('-' * 50)\n",
    "    # print(variation_list)\n",
    "    # print(len(variation_list))\n",
    "    return variation_list\n",
    "def process_one_page(url_of_page,url_of_file):\n",
    "    # 获取HTML文本\n",
    "    res = requests.get(\n",
    "        url_of_page\n",
    "    )\n",
    "\n",
    "    soup = BeautifulSoup(res.text, features=\"html.parser\")\n",
    "\n",
    "    # 获取典故名字列表\n",
    "    allusion_list = [x.get_text() for x in soup.find(attrs={\"id\": \"IndexPanel\"}).find_all(name=\"a\")]\n",
    "\n",
    "    # 获取 ContentPanel 列表\n",
    "    content_list = soup.find(attrs={\"id\": \"ContentPanel\"}).find_all(attrs={\"class\": \"allusion\"})\n",
    "\n",
    "    this_page_list = []\n",
    "\n",
    "    # 遍历 allusion_list 和 content_list ，即遍历这一页的所有典故\n",
    "    for allusion, content in zip(allusion_list, content_list):\n",
    "\n",
    "        # 获取该典故的异形的列表\n",
    "        variation_list = get_variations_list(content)\n",
    "\n",
    "        # 获取一个典故的字典\n",
    "        allusion_dict = {}\n",
    "        allusion_dict[\"allusion\"] = allusion\n",
    "        allusion_dict[\"variation_list\"] = variation_list\n",
    "        \n",
    "        this_page_list.append(allusion_dict)\n",
    "    this_page_df = pd.DataFrame(this_page_list)\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(url_of_file):\n",
    "        # 如果文件存在，追加数据时不保存列名\n",
    "        this_page_df.to_csv(url_of_file, sep='\\t', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        # 如果文件不存在，保存数据并保存列名\n",
    "        this_page_df.to_csv(url_of_file, sep='\\t', index=False)\n",
    "    return this_page_df.shape[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_len = 0\n",
    "    for i in range(0,102,1):\n",
    "        url_of_page = \"https://sou-yun.cn/AllusionsIndex.aspx?page=\" + str(i)\n",
    "        url_of_file = \"D:/软件包/典故的异形数据.csv\"\n",
    "        this_page_len = process_one_page(url_of_page,url_of_file)\n",
    "        all_len+=this_page_len\n",
    "        print(\"爬完了第\" + str(i+1) + \"页的数据，本页共有\" + str(this_page_len) + \"条数据\")\n",
    "    print(\"共有\" + str(all_len) + \"条数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
